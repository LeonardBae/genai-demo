{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d59d527f-1100-45ff-b051-5f7c9029d94d",
      "metadata": {},
      "source": [
        "# Queries with and without Azure OpenAI"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "eb9a9444-dc90-4fc3-aea7-8ee918301aba",
      "metadata": {},
      "source": [
        "지금까지 인덱스의 데이터 소스에서 검색 엔진을 로드했습니다. 이 노트북에서는 몇 가지 예제 쿼리를 시도한 다음 Azure OpenAI 서비스를 사용하여 사용자 쿼리에 대한 올바른 답변을 얻을 수 있는지 살펴보겠습니다.\n",
        "\n",
        "사용자가 콘토소 일렉트로닉스 인사 문서에 대해 질문하면 엔진이 그에 따라 응답하는 것입니다. 이 단일 인덱스 데모는 회사에서 완전히 다른 주제에 대해 서로 다른 유형의 단일 문서를 로드하고 검색 엔진이 가장 관련성이 높은 결과로 응답해야 하는 시나리오를 모방한 것입니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71f6c7e3-9037-4b1e-ae17-1deaa27b9c08",
      "metadata": {},
      "source": [
        "## Set up variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e50b404-a061-49e7-a3c7-c6eabc98ff0f",
      "metadata": {
        "gather": {
          "logged": 1713623499018
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib\n",
        "import requests\n",
        "import random\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "from IPython.display import display, HTML, Markdown\n",
        "from typing import List\n",
        "from operator import itemgetter\n",
        "\n",
        "# LangChain Imports needed\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import ConfigurableField\n",
        "\n",
        "\n",
        "# Our own libraries needed\n",
        "from common.prompts import DOCSEARCH_PROMPT\n",
        "from common.utils import get_search_results\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"credentials.env\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f2c22f8-79ab-405c-95e8-77a1978e53bc",
      "metadata": {
        "gather": {
          "logged": 1713623499496
        }
      },
      "outputs": [],
      "source": [
        "# Setup the Payloads header\n",
        "headers = {'Content-Type': 'application/json','api-key': os.environ['AZURE_SEARCH_KEY']}\n",
        "params = {'api-version': os.environ['AZURE_SEARCH_API_VERSION']}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9297d29b-1f61-4dce-858e-bf4272172dba",
      "metadata": {},
      "source": [
        "## Single-Index Search queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a46e2d3-298a-4708-83de-9e108b1a117a",
      "metadata": {
        "gather": {
          "logged": 1713623499809
        },
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Text-based Indexe that we are going to query (from Notebook 01)\n",
        "index_name = \"cogsrch-index-files\"\n",
        "indexes = [index_name]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1c62ebb2-d7be-4bfb-b1ba-4db86c11839a",
      "metadata": {},
      "source": [
        "건강 보험, 직원에 대한 직무 설명과 같이 Contoso Electronic의 정책에서 답변하거나 다룰 수 있다고 생각되는 질문을 해보세요. 결과를 ChatGPT의 오픈 버전과 비교해 보세요. <br>\n",
        "\n",
        "Azure OpenAI를 사용한 답변은 이러한 게시물에 포함된 정보만 살펴본다는 점을 기억하세요.\n",
        "\n",
        "**Example Questions you can ask**:\n",
        "<font color=\"red\">\n",
        "- What is CLP?\n",
        "- How Markov chains work?\n",
        "- What are some examples of reinforcement learning?\n",
        "- What are the main risk factors for Covid-19?\n",
        "- What medicine reduces inflamation in the lungs?\n",
        "- Why Covid doesn't affect kids that much compared to adults?\n",
        "- Does chloroquine really works against covid?\n",
        "- Who won the 1994 soccer world cup? # This question should yield no answer if the system is correctly grounded\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9b53c14-19bd-451f-aa43-7ad27ccfeead",
      "metadata": {
        "gather": {
          "logged": 1713623500111
        }
      },
      "outputs": [],
      "source": [
        "QUESTION = \"what is the responsibility of Manager of Human Resources?\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6d925eb-7f9c-429e-a62a-4c37d7702caf",
      "metadata": {},
      "source": [
        "### Search on an index\n",
        "\n",
        "#### **Note**:\n",
        "Multi-index can also be used. To use multi-index:<br>\n",
        "The standarize index must be used. In order to standarize the indexes, **there must be 6 mandatory fields present on each index**: `id, title, name, location, chunk, chunkVector`. This is so that each document can be treated the same along the code. Also, **all indexes must have a semantic configuration**.\n",
        "\n",
        "We are going to use Hybrid Queries: Text + Vector Search combined for optimal results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faf2e30f-e71f-4533-ab52-27d048b80a89",
      "metadata": {
        "gather": {
          "logged": 1713623500424
        }
      },
      "outputs": [],
      "source": [
        "agg_search_results = dict()\n",
        "k = 10\n",
        "\n",
        "for index in indexes:\n",
        "    search_payload = {\n",
        "        \"search\": QUESTION, # Text query\n",
        "        \"select\": \"id, title, name, location, chunk\",\n",
        "        \"queryType\": \"semantic\",\n",
        "        \"vectorQueries\": [{\"text\": QUESTION, \"fields\": \"chunkVector\", \"kind\": \"text\", \"k\": k}], # Vector query\n",
        "        \"semanticConfiguration\": \"my-semantic-config\",\n",
        "        \"captions\": \"extractive\",\n",
        "        \"answers\": \"extractive\",\n",
        "        \"count\":\"true\",\n",
        "        \"top\": k\n",
        "    }\n",
        "\n",
        "    r = requests.post(os.environ['AZURE_SEARCH_ENDPOINT'] + \"/indexes/\" + index + \"/docs/search\",\n",
        "                     data=json.dumps(search_payload), headers=headers, params=params)\n",
        "    print(r.status_code)\n",
        "\n",
        "    search_results = r.json()\n",
        "    agg_search_results[index]=search_results\n",
        "    print(\"Index:\", index, \"Results Found: {}, Results Returned: {}\".format(search_results['@odata.count'], len(search_results['value'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7fd0fe5-4ee0-42e2-a920-72b93a407389",
      "metadata": {
        "tags": []
      },
      "source": [
        "### Display the top results (from a search) based on the score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e938337-602d-4b61-8141-b8c92a5d91da",
      "metadata": {
        "gather": {
          "logged": 1713623501718
        }
      },
      "outputs": [],
      "source": [
        "display(HTML('<h4>Top Answers</h4>'))\n",
        "\n",
        "for index,search_results in agg_search_results.items():\n",
        "    for result in search_results['@search.answers']:\n",
        "        if result['score'] > 0.5: # Show answers that are at least 50% of the max possible score=1\n",
        "            display(HTML('<h5>' + 'Answer - score: ' + str(round(result['score'],2)) + '</h5>'))\n",
        "            display(HTML(result['text']))\n",
        "            \n",
        "print(\"\\n\\n\")\n",
        "display(HTML('<h4>Top Results</h4>'))\n",
        "\n",
        "content = dict()\n",
        "ordered_content = OrderedDict()\n",
        "\n",
        "\n",
        "for index,search_results in agg_search_results.items():\n",
        "    for result in search_results['value']:\n",
        "        if result['@search.rerankerScore'] > 1:# Show answers that are at least 25% of the max possible score=4\n",
        "            content[result['id']]={\n",
        "                                    \"title\": result['title'],\n",
        "                                    \"chunk\": result['chunk'], \n",
        "                                    \"name\": result['name'], \n",
        "                                    \"location\": result['location'] ,\n",
        "                                    \"caption\": result['@search.captions'][0]['text'],\n",
        "                                    \"score\": result['@search.rerankerScore'],\n",
        "                                    \"index\": index\n",
        "                                    }\n",
        "    \n",
        "#After results have been filtered we will Sort and add them as an Ordered list\\n\",\n",
        "for id in sorted(content, key= lambda x: content[x][\"score\"], reverse=True):\n",
        "    ordered_content[id] = content[id]\n",
        "    url = str(ordered_content[id]['location']) + os.environ['BLOB_SAS_TOKEN']\n",
        "    title = str(ordered_content[id]['title']) if (ordered_content[id]['title']) else ordered_content[id]['name']\n",
        "    score = str(round(ordered_content[id]['score'],2))\n",
        "    display(HTML('<h5><a href=\"'+ url + '\">' + title + '</a> - score: '+ score + '</h5>'))\n",
        "    display(HTML(ordered_content[id]['caption']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52a6d3e6-afb2-4fa7-96d3-69bc2373ded5",
      "metadata": {},
      "source": [
        "### Comments on Query results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84e02227-6a92-4944-86f8-6c1e38d90fe4",
      "metadata": {},
      "source": [
        "As seen above the semantic re-ranking feature of Azure AI Search service is good. It gives answers (sometimes) and also the top results with the corresponding file and the paragraph where the answers is possible located.\n",
        "\n",
        "Let's see if we can make this better with Azure OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8df3e6d4-9a09-4b0f-b328-238738ccfaec",
      "metadata": {},
      "source": [
        "# Using Azure OpenAI\n",
        "\n",
        "To use OpenAI to get a better answer to our question, the thought process is simple: let's **give the answer and the content of the documents from the search result to the GPT model as context and let it provide a better response**. This is what RAG (Retreival Augmented Generation) is about.\n",
        "\n",
        "Now, before we do this, we need to understand a few things first:\n",
        "\n",
        "1) Chainning and Prompt Engineering\n",
        "2) Embeddings\n",
        "\n",
        "We will use a library call **LangChain** that wraps a lot of boiler plate code.\n",
        "Langchain is one library that does a lot of the prompt engineering for us under the hood, for more information see [here](https://python.langchain.com/en/latest/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eea62a7d-7e0e-4a93-a89c-20c96560c665",
      "metadata": {
        "gather": {
          "logged": 1713623502038
        }
      },
      "outputs": [],
      "source": [
        "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "325d9138-2250-4f6b-bc88-50d7957f8d33",
      "metadata": {},
      "source": [
        "**Important Note**: Starting now, we will utilize OpenAI models. Please ensure that you have deployed the following models within the Azure OpenAI portal:\n",
        "\n",
        "- text-embedding-ada-002 (or newer)\n",
        "- gpt-35-turbo (1106 or newer)\n",
        "- gpt-4-turbo (1106 or newer)\n",
        "\n",
        "Reference for Azure OpenAI models (regions, limits, dimensions, etc): [HERE](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e7c720e-ece1-45ad-9d01-2dfd15c182bb",
      "metadata": {},
      "source": [
        "## A gentle intro to chaining LLMs and prompt engineering"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2bcd7028-5a6c-4296-8c85-4f420d408d69",
      "metadata": {},
      "source": [
        "체인은 LLM, 도구 또는 데이터 전처리 단계에 대한 호출 시퀀스를 나타냅니다.\n",
        "\n",
        "Azure OpenAI는 사용할 수 있는 LLM(공급자)의 한 유형이지만 Cohere, Huggingface 등과 같은 다른 유형도 있습니다.\n",
        "\n",
        "체인은 단순(예: 일반) 또는 전문화(예: 유틸리티)될 수 있습니다.\n",
        "\n",
        "일반 - 가장 간단한 체인은 단일 LLM입니다. 입력 프롬프트와 LLM의 이름을 받은 다음 텍스트 생성(즉, 프롬프트에 대한 출력)에 LLM을 사용합니다. 다음은 예시입니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13df9247-e784-4e04-9475-55e672efea47",
      "metadata": {
        "gather": {
          "logged": 1713623502341
        }
      },
      "outputs": [],
      "source": [
        "COMPLETION_TOKENS = 2500\n",
        "llm = AzureChatOpenAI(deployment_name=os.environ[\"GPT35_DEPLOYMENT_NAME\"], \n",
        "                      temperature=0, \n",
        "                      max_tokens=COMPLETION_TOKENS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3b55adb-6f98-4f15-b67a-9fbba5820560",
      "metadata": {
        "gather": {
          "logged": 1713623502577
        }
      },
      "outputs": [],
      "source": [
        "output_parser = StrOutputParser()\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an assistant that give thorough responses to users.\"),\n",
        "    (\"user\", \"{input}. Give your response in {language}\")\n",
        "])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "6417d052-0035-4635-93e8-2bd3ec50d796",
      "metadata": {},
      "source": [
        " | 기호는 유닉스 파이프 연산자와 유사하며, 서로 다른 컴포넌트를 연결하여 한 컴포넌트의 출력을 다음 컴포넌트의 입력으로 공급하는 역할을 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a37e60-a1ef-4750-a1ec-9e4fe5ba07fa",
      "metadata": {
        "gather": {
          "logged": 1713623502812
        }
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6be6b4df-ee2c-4a0c-8ad3-a672d70f4f8d",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "display(Markdown(chain.invoke({\"input\": QUESTION, \"language\": \"English\"})))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "cd8539d0-a538-4368-82c3-5f91d8370f1e",
      "metadata": {},
      "source": [
        "**참고**: 이 액셀러레이터에서 OpenAI를 처음 사용하는 경우, 리소스를 찾을 수 없음 오류가 발생하면 OpenAI 모델 배포 이름이 위에 설정된 환경 변수 os.environ[\"GPT35_DEPLOYMENT_NAME\"]과 다르기 때문일 가능성이 높습니다."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "50ed014c-0c6b-448c-b995-fe7970b92ad5",
      "metadata": {},
      "source": [
        "이제 간단한 프롬프트를 생성하고 ChatGPT 지식을 사용하여 일반적인 질문에 답하기 위해 체인을 사용하는 방법을 알게 되었습니다!\n",
        "\n",
        "일반 체인을 독립형 체인으로 사용하는 경우는 거의 없다는 점에 유의하는 것이 중요합니다. 더 자주 유틸리티 체인의 빌딩 블록으로 사용됩니다 (다음에 살펴보겠습니다). 또한 주목해야 할 중요한 점은 아직 문서나 Azure 검색 결과를 사용하는 것이 아니라 학습된 데이터에 대한 ChatGPT의 지식만 사용한다는 것입니다."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "12c48038-b1af-4228-8ffb-720e554fd3b2",
      "metadata": {
        "tags": []
      },
      "source": [
        "**두 번째 유형의 체인은 유틸리티입니다.**\n",
        "\n",
        "유틸리티 - 특정 작업을 해결하는 데 도움이 되는 여러 빌딩 블록으로 구성된 전문화된 체인입니다. \n",
        "<br> 예를 들어, LangChain은 일부 엔드투엔드 체인을 지원합니다(예: QnA 문서 검색, 요약 등을 위한 create_retrieval_chain).\n",
        "이 워크샵에서는 더 깊이 파고들기 위해 자체적으로 특정 체인을 구축하고 Azure AI Search의 결과를 개선하는 사용 사례를 해결해 보겠습니다."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "b0454ddb-44d8-4fa9-929a-5e5563dd28f8",
      "metadata": {},
      "source": [
        "하지만 필요한 유틸리티 체인을 다루기 전에 먼저 임베딩과 벡터 검색 및 RAG의 개념을 살펴봅시다.\n",
        "\n",
        "## Embeddings and Vector Search\n",
        "\n",
        "Azure OpenAI 설명서(여기)에서 임베딩은 기계 학습 모델 및 알고리즘에서 쉽게 활용할 수 있는 특별한 형식의 데이터 표현입니다. 임베딩은 텍스트의 의미론적 의미를 밀도 있게 표현한 정보입니다. 각 임베딩은 부동 소수점 숫자로 이루어진 벡터로, 벡터 공간에서 두 임베딩 사이의 거리는 원래 형식의 두 입력 사이의 의미적 유사성과 상관관계가 있습니다. 예를 들어 두 텍스트가 유사하다면 벡터 표현도 유사해야 합니다.\n",
        "\n",
        "### Why Do We Need Vectors?\n",
        "\n",
        "벡터는 여러 가지 이유로 필수적입니다:\n",
        "\n",
        "- **의미론적 풍부함**: 벡터는 텍스트의 의미론적 의미를 수학적 벡터로 변환하여 단순한 키워드 검색으로는 놓칠 수 있는 뉘앙스를 포착합니다. 따라서 언어를 이해하고 처리하는 데 매우 강력합니다.\n",
        "- **인간과 유사한 검색**: 벡터 거리를 이용한 검색은 정확한 단어 일치에만 의존하지 않고 문맥과 의미에 따라 정보를 찾는 인간의 접근 방식을 모방합니다.\n",
        "- **규모의 효율성**: 벡터 표현을 사용하면 대규모 데이터 세트를 효율적으로 처리하고 검색할 수 있습니다. 복잡한 텍스트를 숫자 벡터로 줄임으로써 알고리즘은 방대한 양의 정보를 빠르게 선별할 수 있습니다.\n",
        "\n",
        "### Understanding LLM Tokens' Context Limitation\n",
        "\n",
        "GPT와 같은 대규모 언어 모델(LLM)에는 각 입력에 대한 토큰 제한이 있어 긴 문서나 광범위한 데이터 세트를 다룰 때 문제가 됩니다. 이러한 제한은 제공된 정보의 전체 맥락에 따라 이해하고 응답을 생성하는 모델의 능력을 제한합니다. 따라서 이러한 제한을 효과적으로 관리하고 우회하여 LLM의 성능을 최대한 활용할 수 있는 전략을 수립하는 것이 매우 중요합니다.\n",
        "\n",
        "이 문제를 해결하기 위해 이 솔루션에는 몇 가지 주요 단계가 포함되어 있습니다.\n",
        "\n",
        "1. 문서 세분화: 대용량 문서를 관리하기 쉬운 작은 세그먼트로 세분화합니다. \n",
        "2. 청크의 벡터화: 이러한 세그먼트를 벡터로 변환해 벡터 기반 검색 기술과 호환되도록 합니다. \n",
        "3. 하이브리드 검색: 벡터 및 텍스트 검색 방법을 모두 사용하여 쿼리와 관련하여 가장 관련성이 높은 세그먼트를 찾아냅니다. \n",
        "4. 최적의 컨텍스트 제공: 가장 관련성이 높은 세그먼트를 LLM에 제시하여 토큰 한도 내에서 세부 사항과 간결함 사이의 균형을 유지합니다.\n",
        "\n",
        "우리의 궁극적인 목표는 벡터 인덱스와 하이브리드 검색(벡터 + 텍스트)에만 의존하는 것입니다. 다양한 파일 형식에 대해 OCR을 사용하여 파서를 수동으로 코딩하고 데이터를 인덱스와 동기화하는 스케줄러를 개발할 수도 있지만, 더 효율적인 대안이 있습니다: 바로 자동화된 청킹 전략과 벡터화를 제공하는 Azure AI Search입니다. 'ordered_content' 사전에서 볼 수 있듯이 문서 세분화 및 벡터화는 이미 AI Azure Search에서 완료되었다는 점에 유의해야 합니다. 이 전처리 단계는 후속 작업을 간소화하여 빠른 응답 시간을 보장하고 선택한 OpenAI 모델의 토큰 한도를 준수합니다.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "80e79235-3d8b-4713-9336-5004cc4a1556",
      "metadata": {},
      "source": [
        "따라서 이제 우리가 할 일은 Azure AI 검색 쿼리의 결과가 LLM 컨텍스트 크기에 맞는지 확인한 다음 마법이 작동하도록 내버려두는 것뿐입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12682a1b-df92-49ce-a638-7277103f6cb3",
      "metadata": {
        "gather": {
          "logged": 1713623508323
        }
      },
      "outputs": [],
      "source": [
        "index_name = \"cogsrch-index-files\"\n",
        "indexes = [index_name]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "78a6d6a7-18ef-45b2-a216-3c1f50006593",
      "metadata": {},
      "source": [
        "코드 중복을 피하기 위해 위에서 사용한 많은 코드를 함수에 넣었습니다. 이러한 함수는 common/utils.py 및 common/prompts.py 파일에 있습니다. 이렇게 하면 나중에 빌드할 앱에서 이러한 함수를 사용할 수 있습니다.\n",
        "\n",
        "get_search_results()는 다중 인덱스 검색을 수행하고 문서/청크의 결합된 정렬된 목록을 반환합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bccca45-d1dd-476f-b109-a528b857b6b3",
      "metadata": {
        "gather": {
          "logged": 1713623509989
        }
      },
      "outputs": [],
      "source": [
        "k = 10  # play with this parameter and see the quality of the final answer\n",
        "ordered_results = get_search_results(QUESTION, indexes, k=k, reranker_threshold=1)\n",
        "print(\"Number of results:\",len(ordered_results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9334b4b-a378-4d99-b04b-4762d35e8c0a",
      "metadata": {
        "gather": {
          "logged": 1713623510357
        }
      },
      "outputs": [],
      "source": [
        "top_docs = []\n",
        "for key,value in ordered_results.items():\n",
        "    location = value[\"location\"] if value[\"location\"] is not None else \"\"\n",
        "    top_docs.append(Document(page_content=value[\"chunk\"], metadata={\"source\": location, \"score\":value[\"score\"]}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffed0ea2-1d21-477b-a6cd-20621589e204",
      "metadata": {
        "gather": {
          "logged": 1713623515691
        }
      },
      "outputs": [],
      "source": [
        "chain = (\n",
        "    DOCSEARCH_PROMPT  # Passes the 4 variables above to the prompt template\n",
        "    | llm   # Passes the finished prompt to the LLM\n",
        "    | StrOutputParser()  # converts the output (Runnable object) to the desired output (string)\n",
        ")\n",
        "\n",
        "answer = chain.invoke({\"question\": QUESTION, \"context\":top_docs})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32ba674f-4094-4d32-add4-761d1d79940f",
      "metadata": {
        "gather": {
          "logged": 1713623516049
        }
      },
      "outputs": [],
      "source": [
        "answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7714f38a-daaa-4fc5-a95a-dd025d153216",
      "metadata": {
        "gather": {
          "logged": 1713623516400
        }
      },
      "outputs": [],
      "source": [
        "# Uncomment the below line if you want to inspect the ordered results\n",
        "# ordered_results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "235d4238-df6e-40eb-ab38-4fe6db614acd",
      "metadata": {},
      "source": [
        "Now let's create a Prompt Template that will ground the response only in the chunks retrieve by our hybrid AI Search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f86ed786-aca0-4e25-947b-d9cf3a82665c",
      "metadata": {
        "gather": {
          "logged": 1713623516696
        }
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Answer the question thoroughly, based **ONLY** on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25cba3d1-b5ab-4e28-96b3-ef923d99dc9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time \n",
        "# Creation of our custom chain\n",
        "chain = prompt | llm | output_parser\n",
        "\n",
        "try:\n",
        "    display(Markdown(chain.invoke({\"question\": QUESTION, \"context\": ordered_results})))\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "406c2b91-6752-4ea2-b95c-d1a52dbdd62b",
      "metadata": {},
      "source": [
        "### From GPT-3.5 to GPT-4\n",
        "\n",
        "Now let's see how the response changes if we change to GPT-4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f01705f1-7194-452b-a170-c09ba7752b1d",
      "metadata": {
        "gather": {
          "logged": 1713623519188
        }
      },
      "outputs": [],
      "source": [
        "llm_2 = AzureChatOpenAI(deployment_name=os.environ[\"GPT4_DEPLOYMENT_NAME\"], temperature=0.5, max_tokens=COMPLETION_TOKENS)\n",
        "chain = prompt | llm_2 | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c83bb17-36d3-4eb6-ae6f-9c68f3033d2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "try:\n",
        "    display(Markdown(chain.invoke({\"question\": QUESTION, \"context\": ordered_results})))\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92a28869-13c1-463e-8285-9870e6ac1946",
      "metadata": {},
      "source": [
        "#### As we can see, the model selection MATTERS!\n",
        "\n",
        "We will dive deeper into this later, but for now, **look at the diference between GPT3.5 and GPT4, in quality and in response time**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "417925af-486a-40bc-a290-28c35968c581",
      "metadata": {},
      "source": [
        "# Improving the Prompt and adding citations\n",
        "\n",
        "We could see above that the answer given by GPT3.5 was very simple compared to GPT4, even when the prompt says \"thorough responses to users\". We also could see that there is no citations or references. **How do we know if the answer is grounded on the context or not?**\n",
        "\n",
        "Let's see if these two issues can be improved by Prompt Engineering.<br>\n",
        "On `common/prompts.py` we created a prompt called `DOCSEARCH_PROMPT` check it out!\n",
        "\n",
        "Let's also create a custom Retriever class so we can plug it in easily within the chain building. \n",
        "Note: we can also use the Azure AI Search retriever class [HERE](https://python.langchain.com/docs/integrations/vectorstores/azuresearch), however we want to create a custom Retriever for the following reasons:\n",
        "\n",
        "1) We want to do multi-index searches in one call\n",
        "2) Easier to teach complex concepts of LangChain in this notebook\n",
        "3) We want to use the REST API vs the Python Azure Search SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf31f99-0dfb-423a-81f5-03018e61d9a9",
      "metadata": {
        "gather": {
          "logged": 1713623539608
        }
      },
      "outputs": [],
      "source": [
        "class CustomRetriever(BaseRetriever):\n",
        "    \n",
        "    topK : int\n",
        "    reranker_threshold : int\n",
        "    indexes: List\n",
        "    sas_token: str = None\n",
        "    \n",
        "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        \n",
        "        ordered_results = get_search_results(query, self.indexes, k=self.topK, \n",
        "                                             reranker_threshold=self.reranker_threshold, \n",
        "                                             sas_token=self.sas_token)\n",
        "        top_docs = []\n",
        "        for key,value in ordered_results.items():\n",
        "            location = value[\"location\"] if value[\"location\"] is not None else \"\"\n",
        "            top_docs.append(Document(page_content=value[\"chunk\"], metadata={\"source\": location, \"score\":value[\"score\"]}))\n",
        "\n",
        "        return top_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19b39c79-c827-4437-b58b-6a6fae53b968",
      "metadata": {
        "gather": {
          "logged": 1713623539968
        }
      },
      "outputs": [],
      "source": [
        "# Create the retriever\n",
        "retriever = CustomRetriever(indexes=indexes, topK=k, reranker_threshold=1, sas_token=os.environ['BLOB_SAS_TOKEN'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7aa4f58-4791-40a0-80c5-6582e0574579",
      "metadata": {
        "gather": {
          "logged": 1713623540656
        }
      },
      "outputs": [],
      "source": [
        "# Test retreiver\n",
        "results = retriever.get_relevant_documents(QUESTION)\n",
        "len(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b6546f-b5c5-4168-97fc-2636c50e41c2",
      "metadata": {
        "gather": {
          "logged": 1713623540957
        }
      },
      "outputs": [],
      "source": [
        "# We can create now a dynamically configurable llm object that can change the model at runtime\n",
        "dynamic_llm = AzureChatOpenAI(deployment_name=os.environ[\"GPT35_DEPLOYMENT_NAME\"], \n",
        "                              temperature=0.5, max_tokens=COMPLETION_TOKENS).configurable_alternatives(\n",
        "    # This gives this field an id\n",
        "    # When configuring the end runnable, we can then use this id to configure this field\n",
        "    ConfigurableField(id=\"model\"),\n",
        "    # This sets a default_key.\n",
        "    # If we specify this key, the default LLM  (initialized above) will be used\n",
        "    default_key=\"gpt35\",\n",
        "    # This adds a new option, with name `gpt4`\n",
        "    gpt4=AzureChatOpenAI(deployment_name=os.environ[\"GPT4_DEPLOYMENT_NAME\"], \n",
        "                         temperature=0.5, max_tokens=COMPLETION_TOKENS),\n",
        "    # You can add more configuration options here\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7da2f31-cf5d-4f3a-aad5-67b50b56968e",
      "metadata": {
        "gather": {
          "logged": 1713623541288
        }
      },
      "outputs": [],
      "source": [
        "# Declaration of the chain with the dynamic llm and the new prompt\n",
        "configurable_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | retriever, # Passes the question to the retriever and the results are assign to context\n",
        "        \"question\": itemgetter(\"question\")\n",
        "    }\n",
        "    | DOCSEARCH_PROMPT  # Passes the input variables above to the prompt template\n",
        "    | dynamic_llm   # Passes the finished prompt to the LLM\n",
        "    | StrOutputParser()  # converts the output (Runnable object) to the desired output (string)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b67200e5-d3ae-4c86-9f69-bc7b964ab532",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "try:\n",
        "    display(Markdown(configurable_chain.with_config(configurable={\"model\": \"gpt35\"}).invoke({\"question\": QUESTION})))\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8661b48d-1e57-4a70-9b0a-cc59f9093267",
      "metadata": {},
      "source": [
        "As seen above, we were able to improve the quality and breath of the answer and add citations with only prompt engineering!\n",
        "\n",
        "Let's try again GPT-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efcfac6b-bac2-40c6-9ded-e4ee38e3093f",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "try:\n",
        "    display(Markdown(configurable_chain.with_config(configurable={\"model\": \"gpt4\"}).invoke({\"question\": QUESTION})))\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c93b000a-8d0a-4574-be7c-48d26dfb4c70",
      "metadata": {},
      "source": [
        "#### As you can see the answer from GPT4 is richer, and includes all the relevant chunks. GPT3.5 tends to focus in the first and last chunks only"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6690453b-a9b1-4907-bd43-8c6b3ecba26e",
      "metadata": {},
      "source": [
        "## Adding Streaming to improve user experience and performance\n",
        "\n",
        "It is obvious by now that **GPT4 answers are better quality than GPT3.5**. None are incorrect, but GPT4 is better at understanding the context, following the prompt instructions and on giving a comprehensive answer.\n",
        "\n",
        "One way to make GPT4 look faster is to stream the answer, so the user can see the response as it is typed. To do this, we just simply need to call the method `stream` instead of `invoke`. More on Streaming and Callbacks in later notebooks, but for now, this is one simple way to do it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d250c88-5984-438f-8390-1d93756048ab",
      "metadata": {
        "gather": {
          "logged": 1713623658630
        }
      },
      "outputs": [],
      "source": [
        "for chunk in configurable_chain.with_config(configurable={\"model\": \"gpt4\"}).stream({\"question\": QUESTION}):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f347373a-a5be-473d-b64e-0f6b6dbcd0e0",
      "metadata": {},
      "source": [
        "# Summary\n",
        "##### By using OpenAI, the answers to user questions are way better than taking just the results from Azure AI Search. So the summary is:\n",
        "- Utilizing Azure AI Search, we conduct a single-index hybrid search that identifies the top chunks of documents.\n",
        "- Subsequently, Azure OpenAI utilizes these extracted chunks as context, comprehends the content, and employs it to deliver optimal answers.\n",
        "- Best of two worlds!\n",
        "\n",
        "##### Important observations on this notebook:\n",
        "\n",
        "1) Answers with GPT-3.5 are less quality but way faster\n",
        "2) Answers with GPT-3.5 sometimes failed on provinding citations in the right format\n",
        "3) Answers with GPT-4 are great quality but way slower\n",
        "4) Answers with GPT-4 always provide good and diverse citations in the right format\n",
        "5) Streaming the answers improves the user experience big time!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdc6e2fe-1c34-4952-99ad-14940f022379",
      "metadata": {},
      "source": [
        "# NEXT\n",
        "In the next notebook, we are going to see how we can treat complex and large documents separately, also using Vector Search"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
