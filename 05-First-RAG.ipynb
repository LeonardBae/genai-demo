{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building our First RAG bot - Skill: talk to Search Engine"
      ],
      "metadata": {},
      "id": "76fbaf88-5952-47bf-a68c-85011e49b6de"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have now all the building blocks to build our first Bot that \"talks with my data\". These blocks are:\n",
        "\n",
        "1) A well indexed hybrid (text and vector) engine with my data in chunks -> Azure AI Search\n",
        "2) A good LLM python framework to build LLM Apps -> LangChain\n",
        "3) Quality OpenAI GPT models that understand language and follow instructions -> GPT3.5 and GPT4\n",
        "4) A persisten memory database -> CosmosDB\n",
        "\n",
        "We are missing just one thing: **Agents**.\n",
        "\n",
        "In this Notebook we introduce the concept of Agents and we use it to build or first RAG bot."
      ],
      "metadata": {},
      "id": "967c3b06-c8a0-45db-be9a-974c762ba4b8"
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import asyncio\n",
        "from typing import Dict, List\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import Optional, Type\n",
        "\n",
        "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_core.runnables import ConfigurableField, ConfigurableFieldSpec\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory, CosmosDBChatMessageHistory\n",
        "from langchain.callbacks.manager import AsyncCallbackManagerForToolRun, CallbackManagerForToolRun\n",
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from langchain.tools import BaseTool, StructuredTool, tool\n",
        "\n",
        "#custom libraries that we will use later in the app\n",
        "from common.utils import  GetDocSearchResults_Tool\n",
        "from common.prompts import AGENT_DOCSEARCH_PROMPT\n",
        "\n",
        "from IPython.display import Markdown, HTML, display  \n",
        "\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"credentials.env\")\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1713624044568
        }
      },
      "id": "b64f701d-5b9d-4c7c-b259-c2a515c75961"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1713624044857
        }
      },
      "id": "e4163af7-39d0-43b4-8dad-c13108d22a1d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing: Agents"
      ],
      "metadata": {},
      "id": "33836104-822e-4846-8b81-0de8e24838f1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The implementation of Agents is inspired by two papers: the [MRKL Systems](https://arxiv.org/abs/2205.00445) paper (pronounced â€˜miracleâ€™ ðŸ˜‰) and the [ReAct](https://arxiv.org/abs/2210.03629) paper.\n",
        "\n",
        "Agents are a way to leverage the ability of LLMs to understand and act on prompts. In essence, an Agent is an LLM that has been given a very clever initial prompt. The prompt tells the LLM to break down the process of answering a complex query into a sequence of steps that are resolved one at a time.\n",
        "\n",
        "Agents become really cool when we combine them with â€˜expertsâ€™, introduced in the MRKL paper. Simple example: an Agent might not have the inherent capability to reliably perform mathematical calculations by itself. However, we can introduce an expert - in this case a calculator, an expert at mathematical calculations. Now, when we need to perform a calculation, the Agent can call in the expert rather than trying to predict the result itself. This is actually the concept behind [ChatGPT Pluggins](https://openai.com/blog/chatgpt-plugins).\n",
        "\n",
        "In our case, in order to solve the problem \"How do I build a smart bot that talks to my data\", we need this REACT/MRKL approach, in which we need to instruct the LLM that it needs to use 'experts/tools' in order to read/load/understand/interact with a any particular source of data.\n",
        "\n",
        "Let's create then an Agent that interact with the user and uses a Tool to get the information from the Search engine."
      ],
      "metadata": {},
      "id": "16fc3d38-93f8-4a47-8125-d1bb9f529178"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We start first defining the Tool/Expert"
      ],
      "metadata": {},
      "id": "a7999a06-aff0-4d21-8be7-fe56c70082a8"
    },
    {
      "cell_type": "code",
      "source": [
        "index1_name = \"cogsrch-index-files\"\n",
        "index2_name = \"cogsrch-index-books\"\n",
        "indexes = [index1_name, index2_name]"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1713624045087
        }
      },
      "id": "a862366b-ce9e-44f8-9610-84ec568653ea"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to convert the Retreiver object into a Tool object (\"the expert\"). Check out the Tool `GetDocSearchResults_Tool` in `utils.py`"
      ],
      "metadata": {},
      "id": "077886c8-c5d0-481d-a5f9-f4becf60e0f9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Declare the tools the agent will use"
      ],
      "metadata": {},
      "id": "f73c6ca7-d93b-4961-b90a-08572cad78d8"
    },
    {
      "cell_type": "code",
      "source": [
        "topK=7\n",
        "tools = [GetDocSearchResults_Tool(indexes=indexes, k=5, reranker_th=1, sas_token=os.environ['BLOB_SAS_TOKEN'])]"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1713624045276
        }
      },
      "id": "4a0fd3a0-527c-42e3-a092-46e03d33bd07"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the prompt to use `AGENT_DOCSEARCH_PROMPT` - you can modify this in `prompts.py`! Check it out!"
      ],
      "metadata": {},
      "id": "f9cac295-8be5-4803-8342-6d4e48cd2294"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = AGENT_DOCSEARCH_PROMPT"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1713624045468
        }
      },
      "id": "a44f8df6-a68e-4215-99f3-10119f796c0c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the LLM to use"
      ],
      "metadata": {},
      "id": "5f3ddf18-3f3c-44b4-8af5-1437973da010"
    },
    {
      "cell_type": "code",
      "source": [
        "COMPLETION_TOKENS = 1500\n",
        "llm = AzureChatOpenAI(deployment_name=os.environ[\"GPT35_DEPLOYMENT_NAME\"], temperature=0.5, max_tokens=COMPLETION_TOKENS, streaming=True).configurable_alternatives(\n",
        "    ConfigurableField(id=\"model\"),\n",
        "    default_key=\"gpt35\",\n",
        "    gpt4=AzureChatOpenAI(deployment_name=os.environ[\"GPT4_DEPLOYMENT_NAME\"], temperature=0.5, max_tokens=COMPLETION_TOKENS, streaming=True),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1713624045688
        }
      },
      "id": "5aaaf7f5-ef26-48d8-868d-b53aa4c4f9f4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct the OpenAI Tools agent.\n",
        "> OpenAI API has deprecated functions in favor of tools. The difference between the two is that the tools API allows the model to request that multiple functions be invoked at once, which can reduce response times in some architectures. Itâ€™s recommended to use the tools agent for OpenAI models."
      ],
      "metadata": {},
      "id": "7d527c12-4e18-4f3f-a9ec-8dab4f9ca7b2"
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_openai_tools_agent(llm.with_config(configurable={\"model\": \"gpt35\"}), tools, prompt)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1713624045950
        }
      },
      "id": "6fff2766-defb-45fc-b271-3c811077076b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create an agent executor by passing in the agent and tools"
      ],
      "metadata": {},
      "id": "338336d9-a64a-4602-908a-742b418e4520"
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1713624046154
        }
      },
      "id": "ad6c156f-9a17-4daa-80de-70ce2f55063b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Give it memory - since AgentExecutor is also a Runnable class, we do the same with did on Notebook 5"
      ],
      "metadata": {},
      "id": "252a017c-3b36-43ab-8633-78f4f005d166"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_session_history(session_id: str, user_id: str) -> CosmosDBChatMessageHistory:\n",
        "    cosmos = CosmosDBChatMessageHistory(\n",
        "        cosmos_endpoint=os.environ['AZURE_COSMOSDB_ENDPOINT'],\n",
        "        cosmos_database=os.environ['AZURE_COSMOSDB_NAME'],\n",
        "        cosmos_container=os.environ['AZURE_COSMOSDB_CONTAINER_NAME'],\n",
        "        connection_string=os.environ['AZURE_COMOSDB_CONNECTION_STRING'],\n",
        "        session_id=session_id,\n",
        "        user_id=user_id\n",
        "        )\n",
        "\n",
        "    # prepare the cosmosdb instance\n",
        "    cosmos.prepare_cosmos()\n",
        "    return cosmos"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1713624046357
        }
      },
      "id": "7c013314-afe6-4218-b179-d0f7312d2670"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because cosmosDB needs two fields (an id and a partition), and RunnableWithMessageHistory takes by default only one identifier for memory (session_id), we need to use `history_factory_config` parameter and define the multiple keys for the memory class"
      ],
      "metadata": {},
      "id": "13df017f-3ab7-4943-adc1-3477badf3d3e"
    },
    {
      "cell_type": "code",
      "source": [
        "userid_spec = ConfigurableFieldSpec(\n",
        "            id=\"user_id\",\n",
        "            annotation=str,\n",
        "            name=\"User ID\",\n",
        "            description=\"Unique identifier for the user.\",\n",
        "            default=\"\",\n",
        "            is_shared=True,\n",
        "        )\n",
        "session_id = ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"Unique identifier for the conversation.\",\n",
        "            default=\"\",\n",
        "            is_shared=True,\n",
        "        )"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1713624046587
        }
      },
      "id": "bf93758f-da3b-48fb-9882-91fe327b1751"
    },
    {
      "cell_type": "code",
      "source": [
        "agent_with_chat_history = RunnableWithMessageHistory(\n",
        "    agent_executor,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"question\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[userid_spec,session_id]\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1713624046791
        }
      },
      "id": "52d1aaa6-efca-4512-b680-896dae39a359"
    },
    {
      "cell_type": "code",
      "source": [
        "# configure the session id and user id\n",
        "random_session_id = \"session\"+ str(random.randint(1, 1000))\n",
        "ramdom_user_id = \"user\"+ str(random.randint(1, 1000))\n",
        "\n",
        "config={\"configurable\": {\"session_id\": random_session_id, \"user_id\": ramdom_user_id}}\n",
        "config"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "{'configurable': {'session_id': 'session819', 'user_id': 'user502'}}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1713624047055
        }
      },
      "id": "05c6b489-3db9-4965-9eae-ed2790e62bd7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the Agent!"
      ],
      "metadata": {},
      "id": "3295c54e-a5e2-46f6-99fc-6f76453a877d"
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "agent_with_chat_history.invoke({\"question\": \"Hi, I'm Pablo Marin. What's yours\"}, config=config)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "CPU times: user 416 ms, sys: 13.8 ms, total: 430 ms\nWall time: 2.01 s\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "{'question': \"Hi, I'm Pablo Marin. What's yours\",\n 'history': [],\n 'output': \"Hello Pablo Marin, I'm Jarvis, your personal AI assistant. How can I assist you today?\"}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {},
      "id": "2ac81763-6bcc-4408-9daf-d047a0e2cb08"
    },
    {
      "cell_type": "code",
      "source": [
        "printmd(agent_with_chat_history.invoke(\n",
        "    {\"question\": \"Can I restore my index or service once it's deleted?\"}, \n",
        "    config=config)[\"output\"])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "Yes, in most cases, you can restore an index or service once it's deleted. However, the ability to restore depends on the specific service or platform you are using. Many cloud service providers offer backup and restore functionality for their services, allowing you to recover data and configurations after deletion.\n\nIf you have accidentally deleted an index or service, I recommend checking the documentation or contacting the support team of the service or platform you are using. They will be able to provide you with specific instructions on how to restore your index or service.\n\nIt's important to note that the availability and process of restoring an index or service may vary depending on the service provider and the backup settings you have in place. It's always a good practice to regularly backup your data and configurations to avoid any potential data loss."
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1713624050152
        }
      },
      "id": "cb3fca7e-33a1-40f1-afb0-dee441a1d1d5"
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    printmd(agent_with_chat_history.invoke(\n",
        "        {\"question\": \"Interesting, Can I move, backup, and restore indexes?\"},\n",
        "        config=config)[\"output\"])\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "Yes, you can typically move, backup, and restore indexes depending on the search service or platform you are using. Here are some general guidelines:\n\n1. **Moving Indexes**: Moving indexes typically involves transferring the index data and configurations from one location or service to another. This can be useful when you want to migrate your search index to a different environment or provider. The specific process for moving indexes will depend on the service or platform you are using. It's recommended to consult the documentation or support resources provided by your service provider for detailed instructions on how to move indexes.\n\n2. **Backing up Indexes**: Backing up indexes involves creating a copy of your index data and configurations to ensure they can be restored in case of accidental deletion, data corruption, or other issues. Some search services or platforms provide built-in backup functionality, allowing you to schedule automatic backups or manually create backups on-demand. It's important to regularly back up your indexes to prevent data loss. Again, consult the documentation or support resources of your service provider for specific instructions on how to backup indexes.\n\n3. **Restoring Indexes**: Restoring indexes involves recovering the data and configurations from a backup and bringing them back to a functional state. If you have a backup of your index, you can typically restore it to the same or a different location or service. The process for restoring indexes will depend on the service or platform you are using. Refer to the documentation or support resources provided by your service provider for detailed instructions on how to restore indexes from backups.\n\nIt's worth noting that the specific capabilities and procedures for moving, backing up, and restoring indexes may vary depending on the search service or platform you are using. It's always recommended to follow the guidelines and best practices provided by your service provider to ensure a smooth and successful process."
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1713624056642
        }
      },
      "id": "c430c456-f390-4319-a3b1-bee19da130cf"
    },
    {
      "cell_type": "code",
      "source": [
        "printmd(agent_with_chat_history.invoke({\"question\": \"Thhank you!\"}, config=config)[\"output\"])"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "You're welcome, Pablo Marin! If you have any more questions, feel free to ask. I'm here to help!"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1713624058514
        }
      },
      "id": "9fd54f71-03c9-4332-885b-0d1df942fa88"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Important: there is a limitation of GPT3.5, once we start adding long prompts, and long contexts and thorough answers, or the agent makes multiple searches for multi-step questions, we run out of space!\n",
        "\n",
        "You can minimize this by:\n",
        "- Shorter System Prompt\n",
        "- Smaller chunks (less than the default of 5000 characters)\n",
        "- Reducing topK to bring less relevant chunks\n",
        "\n",
        "However, you ultimately are sacrificing quality to make everything work with GPT3.5 (cheaper and faster model)"
      ],
      "metadata": {},
      "id": "149648ba-945d-4e7d-81f7-a8bca2ac87f2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's add more things we have learned so far: dynamic LLM selection of GPT4 and asyncronous streaming"
      ],
      "metadata": {},
      "id": "41787714-73fd-4336-85f2-bec3abb41eda"
    },
    {
      "cell_type": "code",
      "source": [
        "agent = create_openai_tools_agent(llm.with_config(configurable={\"model\": \"gpt4\"}), tools, prompt) # We select now GPT-4\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)\n",
        "agent_with_chat_history = RunnableWithMessageHistory(agent_executor,get_session_history,input_messages_key=\"question\", \n",
        "                                                     history_messages_key=\"history\", history_factory_config=[userid_spec,session_id])"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1713624058777
        }
      },
      "id": "1511d2c3-97fe-4232-a560-014d0f157008"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In prior notebooks with use the function `.stream()` of the runnable in order to stream the tokens. However if you need to stream individual tokens from the agent or surface steps occuring within tools, you would need to use a combination of `Callbacks` and `.astream()` OR the new `astream_events` API (beta).\n",
        "\n",
        "Letâ€™s use here the astream_events API to stream the following events:\n",
        "\n",
        "    Agent Start with inputs\n",
        "    Tool Start with inputs\n",
        "    Tool End with outputs\n",
        "    Stream the agent final anwer token by token\n",
        "    Agent End with outputs"
      ],
      "metadata": {},
      "id": "7bec5b32-6017-44b9-97e7-34ba3695e688"
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION = \"Tell me more about your last answer, search again multiple times and provide a deeper explanation\""
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1713624058976
        }
      },
      "id": "9600a35e-8d2e-43d0-a334-092b2e8b832c"
    },
    {
      "cell_type": "code",
      "source": [
        "async for event in agent_with_chat_history.astream_events(\n",
        "    {\"question\": QUESTION}, config=config, version=\"v1\",\n",
        "):\n",
        "    kind = event[\"event\"]\n",
        "    if kind == \"on_chain_start\":\n",
        "        if (\n",
        "            event[\"name\"] == \"AgentExecutor\"\n",
        "        ):  # Was assigned when creating the agent with `.with_config({\"run_name\": \"Agent\"})`\n",
        "            print(\n",
        "                f\"Starting agent: {event['name']}\"\n",
        "            )\n",
        "    elif kind == \"on_chain_end\":\n",
        "        if (\n",
        "            event[\"name\"] == \"AgentExecutor\"\n",
        "        ):  # Was assigned when creating the agent with `.with_config({\"run_name\": \"Agent\"})`\n",
        "            print()\n",
        "            print(\"--\")\n",
        "            print(\n",
        "                f\"Done agent: {event['name']}\"\n",
        "            )\n",
        "    if kind == \"on_chat_model_stream\":\n",
        "        content = event[\"data\"][\"chunk\"].content\n",
        "        if content:\n",
        "            # Empty content in the context of OpenAI means\n",
        "            # that the model is asking for a tool to be invoked.\n",
        "            # So we only print non-empty content\n",
        "            print(content, end=\"\")\n",
        "    elif kind == \"on_tool_start\":\n",
        "        print(\"--\")\n",
        "        print(\n",
        "            f\"Starting tool: {event['name']} with inputs: {event['data'].get('input')}\"\n",
        "        )\n",
        "    elif kind == \"on_tool_end\":\n",
        "        print(f\"Done tool: {event['name']}\")\n",
        "        # print(f\"Tool output was: {event['data'].get('output')}\")\n",
        "        print(\"--\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Starting agent: AgentExecutor\n--\nStarting tool: docsearch with inputs: {'query': 'backup search index'}\n--\nStarting tool: docsearch with inputs: {'query': 'restore index after deletion'}\n--\nStarting tool: docsearch with inputs: {'query': 'move search index to another service'}\nDone tool: docsearch\n--\nDone tool: docsearch\n--\nDone tool: docsearch\n--\nBased on the information provided from the Azure AI Search documentation, let's delve deeper into the topics of restoring, backing up, and moving indexes:\n\n1. **Restoring Indexes**: \n   - There is no native support for restoring indexes after deletion in Azure AI Search<sup><a href=\"https://blobstorage2znp775rdhyvo.blob.core.windows.net/books/Azure_Cognitive_Search_Documentation_Overview.pdf?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-04-29T18:11:18Z&st=2024-03-17T10:11:18Z&spr=https&sig=qtSFdHgO4IxArZIDQZbvcc2T7Q4INFsy7XZiIjOqWE0%3D\" target=\"_blank\">[1]</a></sup>.\n   - If you delete an Azure AI Search index or service, it cannot be recovered, and all indexes in the service are deleted permanently<sup><a href=\"https://blobstorage2znp775rdhyvo.blob.core.windows.net/books/Azure_Cognitive_Search_Documentation_Overview.pdf?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-04-29T18:11:18Z&st=2024-03-17T10:11:18Z&spr=https&sig=qtSFdHgO4IxArZIDQZbvcc2T7Q4INFsy7XZiIjOqWE0%3D\" target=\"_blank\">[1]</a></sup>.\n\n2. **Backing Up Indexes**: \n   - Azure AI Search considers indexes as downstream data structures, which means they are populated from other data sources. Because of this, there is no built-in support for backing up indexes. The expectation is that you can rebuild an index from the source data<sup><a href=\"https://blobstorage2znp775rdhyvo.blob.core.windows.net/books/Azure_Cognitive_Search_Documentation_Overview.pdf?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-04-29T18:11:18Z&st=2024-03-17T10:11:18Z&spr=https&sig=qtSFdHgO4IxArZIDQZbvcc2T7Q4INFsy7XZiIjOqWE0%3D\" target=\"_blank\">[1]</a></sup>.\n\n3. **Moving Indexes**: \n   - While there is no native support for porting indexes directly, you can move an index between search services using sample code provided in the Azure AI Search .NET and Python repositories<sup><a href=\"https://blobstorage2znp775rdhyvo.blob.core.windows.net/books/Azure_Cognitive_Search_Documentation_Overview.pdf?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-04-29T18:11:18Z&st=2024-03-17T10:11:18Z&spr=https&sig=qtSFdHgO4IxArZIDQZbvcc2T7Q4INFsy7XZiIjOqWE0%3D\" target=\"_blank\">[1]</a></sup>.\n\n4. **Vector Search**: \n   - Vector search is a technique used in Azure AI Search to find the most similar documents by comparing their vector representations. It is useful for identifying similar content even without explicit keyword matches<sup><a href=\"https://blobstorage2znp775rdhyvo.blob.core.windows.net/books/Azure_Cognitive_Search_Documentation_Overview.pdf?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-04-29T18:11:18Z&st=2024-03-17T10:11:18Z&spr=https&sig=qtSFdHgO4IxArZIDQZbvcc2T7Q4INFsy7XZiIjOqWE0%3D\" target=\"_blank\">[1]</a></sup>.\n\n5. **SQL Database Replicas**: \n   - When using Azure SQL Database as a data source for indexing, there are no restrictions on using primary or secondary replicas to build an index from scratch. However, for incremental updates, only the primary replica should be used due to guaranteed change tracking<sup><a href=\"https://blobstorage2znp775rdhyvo.blob.core.windows.net/books/Azure_Cognitive_Search_Documentation_Overview.pdf?sv=2022-11-02&ss=bfqt&srt=sco&sp=rwdlacupiytfx&se=2024-04-29T18:11:18Z&st=2024-03-17T10:11:18Z&spr=https&sig=qtSFdHgO4IxArZIDQZbvcc2T7Q4INFsy7XZiIjOqWE0%3D\" target=\"_blank\">[1]</a></sup>.\n\nThe information provided here is specific to Azure AI Search and may not be applicable to other search services or platforms. It is essential to consult the documentation or support resources of your specific service provider for accurate and detailed instructions regarding these operations.\n--\nDone agent: AgentExecutor\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1713624205700
        }
      },
      "id": "3808fa33-05bb-4f5d-9ab9-7159f6db62a8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Note: Try to run this last question with GPT3.5 and see how you are going to run out of token space in the LLM"
      ],
      "metadata": {},
      "id": "4b41bba7-18df-4ab8-b4f6-60368160d348"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "We just built our first RAG BOT!.\n",
        "\n",
        "- We learned that **Agents + Tools are the best way to go about building Bots**. <br>\n",
        "- We converted the Azure Search retriever into a Tool using the function `GetDocSearchResults_Tool` in `utils.py`\n",
        "- We learned about the events API (Beta), one way to stream the answer from agents\n",
        "- We learned that for comprehensive, quality answers we will run out of space with GPT3.5. GPT4 then becomes necessary.\n"
      ],
      "metadata": {},
      "id": "e0ec64bf-fe24-42fc-8dde-4d478f0af21e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEXT\n",
        "\n",
        "Now that we have a bot with one skill (Document Search), let's build more skills!. In the next Notebook, we will guide you on how we stick everything together. How do we use the features of all notebooks and create a brain agent that can respond to any request accordingly."
      ],
      "metadata": {},
      "id": "56306506-d53d-4d43-93e2-a9300ed2a3ee"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}