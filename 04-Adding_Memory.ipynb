{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "01a8b5c0-87cb-4302-8e3c-dc809d0039fb",
      "metadata": {},
      "source": [
        "# Understanding Memory in LLMs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "a2f73380-6395-4e9f-9c83-3f47a5d7e292",
      "metadata": {},
      "source": [
        "이전 노트북에서는 OpenAI 모델이 Azure AI 검색 쿼리의 결과를 향상시키는 방법을 성공적으로 살펴봤습니다. \n",
        "하지만 LLM과 대화에 참여하는 방법은 아직 발견하지 못했습니다. 예를 들어, [Bing Chat](http://chat.bing.com/)을 사용하면 이전 응답을 이해하고 참조할 수 있기 때문에 이것이 가능합니다.\n",
        "LLM(대규모 언어 모델)에 메모리가 있다는 일반적인 오해가 있습니다. 이는 사실이 아닙니다. 지식을 보유하고 있기는 하지만 이전에 질문했던 정보를 기억하지는 못합니다.\n",
        "이 노트북의 목표는 프롬프트와 문맥을 활용하여 LLM에 효과적으로 \"메모리를 부여\"하는 방법을 설명하는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "733c782e-204c-47d0-8dae-c9df7091ab23",
      "metadata": {
        "gather": {
          "logged": 1713623975213
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory, CosmosDBChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from operator import itemgetter\n",
        "from typing import List\n",
        "\n",
        "from IPython.display import Markdown, HTML, display  \n",
        "\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "\n",
        "#custom libraries that we will use later in the app\n",
        "from common.utils import CustomAzureSearchRetriever, get_answer\n",
        "from common.prompts import DOCSEARCH_PROMPT\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv(\"credentials.env\")\n",
        "\n",
        "import logging\n",
        "\n",
        "# Get the root logger\n",
        "logger = logging.getLogger()\n",
        "# Set the logging level to a higher level to ignore INFO messages\n",
        "logger.setLevel(logging.WARNING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bc63c55-a57d-49a7-b6c7-0f18bca8199e",
      "metadata": {
        "gather": {
          "logged": 1713623975524
        }
      },
      "outputs": [],
      "source": [
        "# Set the ENV variables that Langchain needs to connect to Azure OpenAI\n",
        "os.environ[\"OPENAI_API_VERSION\"] = os.environ[\"AZURE_OPENAI_API_VERSION\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "3dc72b22-11c2-4df0-91b8-033d01829663",
      "metadata": {},
      "source": [
        "### Let's start with the basics\n",
        "아주 간단한 예제를 사용하여 Azure OpenAI의 GPT 모델에 메모리가 있는지 확인해 보겠습니다. 다시 langchain을 사용하여 코드를 간소화하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eef5dc9-8b80-4085-980c-865fa41fa1f6",
      "metadata": {
        "gather": {
          "logged": 1713623976317
        }
      },
      "outputs": [],
      "source": [
        "QUESTION = \"Tell me some use cases for reinforcement learning\"\n",
        "FOLLOW_UP_QUESTION = \"What was my prior question?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a00181d5-bd76-4ce4-a256-75ac5b58c60f",
      "metadata": {
        "gather": {
          "logged": 1713623976745
        }
      },
      "outputs": [],
      "source": [
        "COMPLETION_TOKENS = 1000\n",
        "# Create an OpenAI instance\n",
        "llm = AzureChatOpenAI(deployment_name=os.environ[\"GPT35_DEPLOYMENT_NAME\"], \n",
        "                      temperature=0.5, max_tokens=COMPLETION_TOKENS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9502d0f1-fddf-40d1-95d2-a1461dcc498a",
      "metadata": {
        "gather": {
          "logged": 1713623977029
        }
      },
      "outputs": [],
      "source": [
        "# We create a very simple prompt template, just the question as is:\n",
        "output_parser = StrOutputParser()\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an assistant that give thorough responses to users.\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5c9903e-15c7-4e05-87a1-58e5a7917ba2",
      "metadata": {
        "gather": {
          "logged": 1713623978783
        }
      },
      "outputs": [],
      "source": [
        "# Let's see what the GPT model responds\n",
        "chain = prompt | llm | output_parser\n",
        "response_to_initial_question = chain.invoke({\"input\": QUESTION})\n",
        "display(Markdown(response_to_initial_question))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99acaf3c-ce68-4b87-b24a-6065b15ff9a8",
      "metadata": {
        "gather": {
          "logged": 1713623979473
        }
      },
      "outputs": [],
      "source": [
        "#Now let's ask a follow up question\n",
        "printmd(chain.invoke({\"input\": FOLLOW_UP_QUESTION}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3e1c143-c95f-4566-a8b4-af8c42f08dd2",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "As you can see, it doesn't remember what it just responded, sometimes it responds based only on the system prompt, or just randomly. This proof that the LLM does NOT have memory and that we need to give the memory as a a conversation history as part of the prompt, like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0946ce71-6285-432e-b011-9c2dc1ba7b8a",
      "metadata": {
        "gather": {
          "logged": 1713623979764
        }
      },
      "outputs": [],
      "source": [
        "hist_prompt = ChatPromptTemplate.from_template(\n",
        "\"\"\"\n",
        "    {history}\n",
        "    Human: {question}\n",
        "    AI:\n",
        "\"\"\"\n",
        ")\n",
        "chain = hist_prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d088e51-e5eb-4143-b87d-b2be429eb864",
      "metadata": {
        "gather": {
          "logged": 1713623979982
        }
      },
      "outputs": [],
      "source": [
        "Conversation_history = \"\"\"\n",
        "Human: {question}\n",
        "AI: {response}\n",
        "\"\"\".format(question=QUESTION, response=response_to_initial_question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d99e34ad-5539-44dd-b080-3ad05efd2f01",
      "metadata": {
        "gather": {
          "logged": 1713623980279
        }
      },
      "outputs": [],
      "source": [
        "printmd(chain.invoke({\"history\":Conversation_history, \"question\": FOLLOW_UP_QUESTION}))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "045e5af6-55d6-4353-b3f6-3275c95db00a",
      "metadata": {},
      "source": [
        "**Bingo!**, so we now know how to create a chatbot using LLMs, we just need to keep the state/history of the conversation and pass it as context every time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eafd1694-0077-4aa8-bd01-e9f763ce36a3",
      "metadata": {},
      "source": [
        "## Now that we understand the concept of memory via adding history as a context, let's go back to our GPT Smart Search engine"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9787ffb6-2b11-4b03-92fc-9443cd1f2ab9",
      "metadata": {},
      "source": [
        "Langchain 웹사이트에서 발췌:\n",
        "    \n",
        "메모리 시스템은 읽기와 쓰기라는 두 가지 기본 작업을 지원해야 합니다. 모든 체인은 특정 입력을 기대하는 몇 가지 핵심 실행 로직을 정의한다는 점을 기억하세요. 이러한 입력 중 일부는 사용자가 직접 제공하지만, 일부는 메모리에서 제공될 수도 있습니다. 체인은 주어진 실행에서 메모리 시스템과 두 번 상호 작용합니다.\n",
        "\n",
        "    초기 사용자 입력을 받은 후 코어 로직을 실행하기 전에 체인은 메모리 시스템에서 읽고 사용자 입력을 보강합니다.\n",
        "    핵심 로직을 실행한 후 답을 반환하기 전에 체인은 현재 실행의 입력과 출력을 메모리에 기록하여 향후 실행에서 참조할 수 있도록 합니다.\n",
        "    \n",
        "따라서 이 프로세스는 응답에 지연을 추가하지만 필요한 지연입니다 :)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f36e8f14-e566-4ae9-a7d4-6dee7f469dad",
      "metadata": {},
      "source": [
        "![image](https://python.langchain.com/assets/images/memory_diagram-0627c68230aa438f9b5419064d63cbbc.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef9f459b-e8b8-40b9-a94d-80c079968594",
      "metadata": {
        "gather": {
          "logged": 1713623980559
        }
      },
      "outputs": [],
      "source": [
        "index1_name = \"cogsrch-index-files\"\n",
        "index2_name = \"cogsrch-index-books\"\n",
        "indexes = [index1_name, index2_name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b01852c2-6192-496c-adff-4270f9380469",
      "metadata": {
        "gather": {
          "logged": 1713623980785
        }
      },
      "outputs": [],
      "source": [
        "# Initialize our custom retriever \n",
        "retriever = CustomAzureSearchRetriever(indexes=indexes, topK=10, reranker_threshold=1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "633937e8-18e6-43f2-b4d5-fc36157a4d97",
      "metadata": {},
      "source": [
        "\n",
        "prompts.py를 자세히 살펴보면 `DOCSEARCH_PROMPT`에 `history`라는 선택적 변수가 있습니다. 이제 이 변수를 사용할 때입니다. 기본적으로 프롬프트에 대화를 삽입하여 LLM이 응답하기 전에 이를 인식할 수 있도록 하는 플레이스 홀더입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "035fa6e6-226c-400f-a504-30255385f43b",
      "metadata": {},
      "source": [
        "**Now let's add memory to it:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c8c9381-08d0-4808-9ab1-78156ca1be6e",
      "metadata": {
        "gather": {
          "logged": 1713623981027
        }
      },
      "outputs": [],
      "source": [
        "store = {} # Our first memory will be a dictionary in memory\n",
        "\n",
        "# We have to define a custom function that takes a session_id and looks somewhere\n",
        "# (in this case in a dictionary in memory) for the conversation\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48ff51e1-2b1e-4c67-965d-1c2e2f55e005",
      "metadata": {
        "gather": {
          "logged": 1713623981264
        }
      },
      "outputs": [],
      "source": [
        "# We use our original chain with the retriever but removing the StrOutputParser\n",
        "chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | retriever, \n",
        "        \"question\": itemgetter(\"question\"),\n",
        "        \"history\": itemgetter(\"history\")\n",
        "    }\n",
        "    | DOCSEARCH_PROMPT\n",
        "    | llm\n",
        ")\n",
        "\n",
        "## Then we pass the above chain to another chain that adds memory to it\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"question\",\n",
        "    history_messages_key=\"history\",\n",
        ") | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e582915-243f-42cb-bb1e-c35a20ee0b9f",
      "metadata": {
        "gather": {
          "logged": 1713623981503
        }
      },
      "outputs": [],
      "source": [
        "# This is where we configure the session id\n",
        "config={\"configurable\": {\"session_id\": \"abc123\"}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d544a11",
      "metadata": {
        "gather": {
          "logged": 1713623981771
        }
      },
      "outputs": [],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff493b1-b133-4880-a040-e80f7460e7af",
      "metadata": {},
      "source": [
        "Notice below, that we are adding a `history` variable in the call. This variable will hold the chat historywithin the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d91a7ff4-6148-459d-917c-37302805dd09",
      "metadata": {
        "gather": {
          "logged": 1713623991074
        }
      },
      "outputs": [],
      "source": [
        "printmd(chain_with_history.invoke({\"question\": QUESTION}, config=config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25dfc233-450f-4671-8f1c-0b446e46f048",
      "metadata": {
        "gather": {
          "logged": 1713623994502
        }
      },
      "outputs": [],
      "source": [
        "# Remembers\n",
        "printmd(chain_with_history.invoke({\"question\": FOLLOW_UP_QUESTION},config=config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c67073c2-9a82-4e44-a9e2-48fe868c1634",
      "metadata": {
        "gather": {
          "logged": 1713623997488
        }
      },
      "outputs": [],
      "source": [
        "# Remembers\n",
        "printmd(chain_with_history.invoke({\"question\": \"Thank you! Good bye\"},config=config))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87405173",
      "metadata": {},
      "source": [
        "## Using CosmosDB as persistent memory\n",
        "\n",
        "In previous cell we have added local RAM memory to our chatbot. However, it is not persistent, it gets deleted once the app user's session is terminated. It is necessary then to use a Database for persistent storage of each of the bot user conversations, not only for Analytics and Auditing, but also if we wish to provide recommendations in the future. \n",
        "\n",
        "Here we will store the conversation history into CosmosDB for future auditing purpose.\n",
        "We will use a class in LangChain use CosmosDBChatMessageHistory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d87cc7c6-5ef1-4492-b133-9f63a392e223",
      "metadata": {
        "gather": {
          "logged": 1713623997734
        }
      },
      "outputs": [],
      "source": [
        "# Create the function to retrieve the conversation\n",
        "\n",
        "def get_session_history(session_id: str, user_id: str) -> CosmosDBChatMessageHistory:\n",
        "    cosmos = CosmosDBChatMessageHistory(\n",
        "        cosmos_endpoint=os.environ['AZURE_COSMOSDB_ENDPOINT'],\n",
        "        cosmos_database=os.environ['AZURE_COSMOSDB_NAME'],\n",
        "        cosmos_container=os.environ['AZURE_COSMOSDB_CONTAINER_NAME'],\n",
        "        connection_string=os.environ['AZURE_COMOSDB_CONNECTION_STRING'],\n",
        "        session_id=session_id,\n",
        "        user_id=user_id\n",
        "        )\n",
        "\n",
        "    # prepare the cosmosdb instance\n",
        "    cosmos.prepare_cosmos()\n",
        "    return cosmos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94f4179b-c1c7-49da-9c80-a42c275ed4d6",
      "metadata": {
        "gather": {
          "logged": 1713623997961
        }
      },
      "outputs": [],
      "source": [
        "chain_with_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"question\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"user_id\",\n",
        "            annotation=str,\n",
        "            name=\"User ID\",\n",
        "            description=\"Unique identifier for the user.\",\n",
        "            default=\"\",\n",
        "            is_shared=True,\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"Unique identifier for the conversation.\",\n",
        "            default=\"\",\n",
        "            is_shared=True,\n",
        "        ),\n",
        "    ],\n",
        ") | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cf1f1f0-6e46-4136-9f33-4e46617b7d4f",
      "metadata": {
        "gather": {
          "logged": 1713623998197
        }
      },
      "outputs": [],
      "source": [
        "# This is where we configure the session id and user id\n",
        "random_session_id = \"session\"+ str(random.randint(1, 1000))\n",
        "ramdom_user_id = \"user\"+ str(random.randint(1, 1000))\n",
        "\n",
        "config={\"configurable\": {\"session_id\": random_session_id, \"user_id\": ramdom_user_id}}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b20c00c-4098-4970-84e5-f71ea7615c65",
      "metadata": {
        "gather": {
          "logged": 1713623998502
        }
      },
      "outputs": [],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e3c32f4-f883-4045-91f9-ca317c2d01fe",
      "metadata": {
        "gather": {
          "logged": 1713624011048
        }
      },
      "outputs": [],
      "source": [
        "printmd(chain_with_history.invoke({\"question\": QUESTION}, config=config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e29643b-a531-4117-8e85-9c88a625cf02",
      "metadata": {
        "gather": {
          "logged": 1713624014588
        }
      },
      "outputs": [],
      "source": [
        "# Remembers\n",
        "printmd(chain_with_history.invoke({\"question\": FOLLOW_UP_QUESTION},config=config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50146f05-5ef6-484f-a8ec-9631643054f2",
      "metadata": {
        "gather": {
          "logged": 1713624018535
        }
      },
      "outputs": [],
      "source": [
        "# Remembers\n",
        "printmd(chain_with_history.invoke(\n",
        "    {\"question\": \"Can you tell me a one line summary of our conversation?\"},\n",
        "    config=config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bc02369-904c-4063-93e1-fff24fe6a3ab",
      "metadata": {
        "gather": {
          "logged": 1713624022515
        }
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    printmd(chain_with_history.invoke(\n",
        "    {\"question\": \"Thank you very much!\"},\n",
        "    config=config))\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87d60faa-1446-4c07-8970-0f9712c33b2f",
      "metadata": {},
      "outputs": [],
      "source": [
        "printmd(chain_with_history.invoke(\n",
        "    {\"question\": \"I do have one more question, why did you give me a one line summary?\"},\n",
        "    config=config))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfe748aa-6116-4a7a-97e6-f1c680dd23ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "printmd(chain_with_history.invoke(\n",
        "    {\"question\": \"why not 2?\"},\n",
        "    config=config))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc5ac98",
      "metadata": {},
      "source": [
        "#### Let's check our Azure CosmosDB to see the whole conversation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5e30694-ae2a-47bb-a5c7-db51ecdbba1e",
      "metadata": {},
      "source": [
        "![CosmosDB Memory](./images/cosmos-chathistory.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6789cada-23a3-451a-a91a-0906ceb0bd14",
      "metadata": {},
      "source": [
        "# Summary\n",
        "##### Adding memory to our application allows the user to have a conversation, however this feature is not something that comes with the LLM, but instead, memory is something that we must provide to the LLM in form of context of the question.\n",
        "\n",
        "We added persitent memory using CosmosDB.\n",
        "\n",
        "We also can notice that the current chain that we are using is smart, but not that much. Although we have given memory to it, it searches for similar docs everytime, regardless of the input. This doesn't seem efficient, but regardless, we are very close to finish our first RAG-talk to your data Bot.\n",
        "\n",
        "\n",
        "## <u>Important Note</u>:<br>\n",
        "As we proceed, while all the code will remain compatible with GPT-3.5 models (1106 or newer), we highly recommend transitioning to GPT-4. Here's why:\n",
        "\n",
        "**GPT-3.5-Turbo** can be likened to a 7-year-old child. You can provide it with concise instructions, but it struggles sometimes to follow them accurately (not too reliable). Additionally, its limited \"memory\" (token context) can make sustained conversations challenging. Its response are also simple not deep.\n",
        "\n",
        "**GPT-4-Turbo** exhibits the capabilities of a 10-12-year-old child. It possesses enhanced reasoning skills, consistently adheres to instructions and its answers are beter. It has extended memory retention (larger context size) for instructions, and it excels at following them. Its responses are deep and thorough.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c629ebf4-aced-45b7-a6a2-315810d37d48",
      "metadata": {},
      "source": [
        "# NEXT\n",
        "We know now how to do a Smart Search Engine that can power a chatbot!! great!\n",
        "\n",
        "In the next notebook 6, we are going to build our first RAG bot. In order to do this we will introduce the concept of Agents."
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
